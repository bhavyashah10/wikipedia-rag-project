# Wikipedia RAG System

A locally-hosted RAG (Retrieval Augmented Generation) system using Wikipedia as knowledge base, with Ollama for LLM serving and MCP for agentic capabilities.

## ğŸ¯ Project Goals

- **Download & Process**: Full Wikipedia dump (~50GB) â†’ Clean, searchable text chunks
- **Vector Search**: FAISS-powered semantic search over Wikipedia content
- **Local LLM**: Ollama + OpenWebUI for private, offline AI assistance  
- **Agentic Tools**: MCP (Model Context Protocol) for autonomous reasoning and tool use
- **Self-Contained**: Complete knowledge system running locally on MacBook Air M1

## ğŸ—ï¸ Architecture

```
Wikipedia XML â†’ Parser â†’ Chunker â†’ Embeddings â†’ FAISS Index
                                                      â†“
                                              RAG Retrieval
                                                      â†“
                                            Ollama LLM + MCP Agents
```

## ğŸ“Š Current Progress

- âœ… **Wikipedia Download**: Simple English Wikipedia (200MB â†’ 800MB uncompressed)
- âœ… **XML Parsing**: 262,105 articles extracted and cleaned
- âœ… **Text Chunking**: 569,456 chunks created (avg 2.2 chunks/article)
- âœ… **Embedding Generation**: 569K chunks â†’ 384-dim vectors (834MB)
- âœ… **FAISS Index**: Semantic search over 569K vectors (sub-second queries)
- ğŸ”„ **Next**: Ollama LLM integration for RAG conversations
- ğŸ”„ **Next**: OpenWebUI web interface
- ğŸ”„ **Next**: MCP agentic layer

## ğŸš€ Quick Start

### Prerequisites
- MacBook Air M1 (8GB RAM, 256GB+ storage)
- Python 3.9+
- ~150GB free space for full Wikipedia (current test uses ~5GB)

### Setup
```bash
git clone https://github.com/bhavyashah10/wikipedia-rag-project.git
cd wikipedia-rag-project

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Test setup
python test_setup.py
```

### Run Wikipedia Processing
```bash
# Download Simple English Wikipedia (for testing)
curl -O https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles.xml.bz2
mv simplewiki-latest-pages-articles.xml.bz2 data/raw/

# Parse articles from XML
python test_parser.py

# Create text chunks
python src/data_processing/text_chunker.py

# Generate embeddings (10-15 minutes)
python src/embeddings/embedding_generator.py

# Build FAISS index for search
python src/retrieval/faiss_indexer.py
```

### Test the RAG System
```bash
# Test semantic search
python src/retrieval/faiss_indexer.py

# Example queries that work:
# - "What is artificial intelligence?"
# - "How do computers work?" 
# - "Tell me about space exploration"
# - "What is machine learning?"
```

## ğŸ“ Project Structure

```
wikipedia-rag-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Wikipedia XML dumps
â”‚   â”œâ”€â”€ processed/              # Clean articles & chunks (569K chunks)
â”‚   â””â”€â”€ embeddings/             # FAISS indices & vectors (1.6GB)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processing/        # XML parser & text chunker
â”‚   â”œâ”€â”€ embeddings/             # Embedding generation (sentence-transformers)
â”‚   â”œâ”€â”€ retrieval/              # FAISS search & RAG
â”‚   â”œâ”€â”€ llm_integration/        # Ollama interface (coming next)
â”‚   â””â”€â”€ mcp_agents/             # MCP tools & agents (coming next)
â”œâ”€â”€ config/                     # YAML configurations
â”œâ”€â”€ notebooks/                  # Jupyter experiments
â””â”€â”€ logs/                       # Application logs
```

## âš™ï¸ Configuration

See `config/config.yaml` for:
- Processing parameters (chunk size, overlap, filters)
- Embedding model settings (sentence-transformers)
- FAISS index configuration
- LLM settings (Ollama models)
- RAG retrieval parameters

## ğŸ§ª Current Test Results

**Simple English Wikipedia Processing:**
- **Input**: 200MB compressed XML
- **Parsed**: 262,105 articles
- **Chunks**: 569,456 text segments
- **Average**: 2.2 chunks per article
- **Processing Time**: ~5 minutes on M1 MacBook Air

**Embedding Generation:**
- **Model**: sentence-transformers/all-MiniLM-L6-v2
- **Vectors**: 569,456 Ã— 384 dimensions
- **Size**: 834MB embeddings + 132MB metadata
- **Processing Time**: ~15 minutes on M1 MacBook Air

**FAISS Search Performance:**
- **Index Size**: 834MB (cosine similarity, flat index)
- **Search Speed**: Sub-second queries over 569K vectors
- **Quality**: Excellent semantic relevance for test queries
- **Memory Usage**: ~2GB RAM during search

## ğŸ› ï¸ Tech Stack

- **Data Processing**: Python, lxml, BeautifulSoup
- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)
- **Vector DB**: FAISS (CPU-optimized for M1)
- **LLM Serving**: Ollama (Llama2-7B, Mistral-7B)
- **Web Interface**: OpenWebUI
- **Agents**: MCP (Model Context Protocol)
- **Hardware**: Optimized for Apple Silicon M1

## ğŸ“ˆ Next Steps

1. **Ollama Integration** - Local LLM serving (Llama2-7B, Mistral-7B)
2. **RAG Pipeline** - Connect FAISS search to LLM responses
3. **OpenWebUI** - Web-based chat interface
4. **MCP Agents** - Tool use and autonomous reasoning
5. **Scale to Full Wikipedia** - Process complete English Wikipedia (~6.7M articles)

## ğŸ’¡ Example Search Results

The system already demonstrates excellent semantic search:

**Query: "What is artificial intelligence?"**
- âœ… Returns actual AI definition articles
- âœ… Similarity scores: 0.540-0.640 (high relevance)
- âœ… Sub-second response time

**Query: "How do computers work?"**  
- âœ… Finds computer architecture explanations
- âœ… Returns technical details about processors, circuits
- âœ… Perfect semantic matching (not just keyword search)

## ğŸ”§ Hardware Considerations

**Current (Simple Wiki + RAG Search):**
- Storage: ~5GB total (chunks + embeddings + index)
- RAM: ~2GB during search, ~4GB during embedding generation
- Processing: ~20 minutes total setup time

**Full Wikipedia (Estimated):**
- Storage: ~120-150GB total  
- RAM: ~4-6GB during processing
- Processing: ~2-4 hours total setup time
